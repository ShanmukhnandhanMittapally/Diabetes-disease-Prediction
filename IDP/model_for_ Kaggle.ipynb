{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.sciencedirect.com/science/article/pii/S1877050920300557 The paper presents a diabetes prediction model using machine learning to improve diagnostic accuracy. It implements algorithms like Logistic Regression, and others, with data preprocessing, clustering, and model evaluation.\n",
    "The dataset consists of 800 records with 10 attributes, including features like Number of Pregnancies, Glucose Level, Blood Pressure, BMI, Age, and Job Type.\n",
    "\n",
    "- https://www.sciencedirect.com/science/article/pii/S2405959521000205 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Approach\n",
    "  Dataset taken from kaggle https://www.kaggle.com/datasets/saurabh00007/diabetescsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handles missing values, especially for attributes that cannot be zero (e.g., Glucose Level, Blood Pressure). The dataset is scaled to normalize values for better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed. Cleaned data saved to cleaned_diabetes_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "file_path = 'diabetes.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Replace zeros with NaN and fill with median values\n",
    "columns_with_missing_values = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for col in columns_with_missing_values:\n",
    "    data[col] = data[col].replace(0, np.nan)  # Replace 0 with NaN\n",
    "    data[col] = data[col].fillna(data[col].median())  # Fill NaN with median value\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = data.columns.drop('Outcome') \n",
    "data[scaled_columns] = scaler.fit_transform(data[scaled_columns])\n",
    "\n",
    "# Save the cleaned and scaled data to a new CSV file\n",
    "cleaned_file_path = 'cleaned_diabetes_data.csv'\n",
    "data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(\"Data preprocessing completed. Cleaned data saved to\", cleaned_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[82 17]\n",
      " [21 34]]\n",
      "\n",
      "Accuracy: 75.32%\n",
      "Precision: 66.67%\n",
      "Recall: 61.82%\n",
      "F1 Score: 64.15%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[79 21]\n",
      " [28 26]]\n",
      "\n",
      "Accuracy: 68.18%\n",
      "Precision: 55.32%\n",
      "Recall: 48.15%\n",
      "F1 Score: 51.49%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure no missing values (security measure for cleaner data)\n",
    "if data.isnull().values.any():\n",
    "    data = data.dropna()\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[88 12]\n",
      " [22 32]]\n",
      "\n",
      "Accuracy: 77.92%\n",
      "Precision: 72.73%\n",
      "Recall: 59.26%\n",
      "F1 Score: 65.31%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure no missing values (security measure for cleaner data)\n",
    "if data.isnull().values.any():\n",
    "    data = data.dropna()\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[82 18]\n",
      " [28 26]]\n",
      "\n",
      "Accuracy: 70.13%\n",
      "Precision: 59.09%\n",
      "Recall: 48.15%\n",
      "F1 Score: 53.06%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the SVM model\n",
    "svm_model = SVC(kernel='linear')  # Using a linear kernel\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[83 17]\n",
      " [21 33]]\n",
      "\n",
      "Accuracy: 75.32%\n",
      "Precision: 66.00%\n",
      "Recall: 61.11%\n",
      "F1 Score: 63.46%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors (you can adjust this)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[74 26]\n",
      " [20 34]]\n",
      "\n",
      "Accuracy: 70.13%\n",
      "Precision: 56.67%\n",
      "Recall: 62.96%\n",
      "F1 Score: 59.65%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the Naive Bayes model\n",
    "nb_model = GaussianNB()  # Using Gaussian Naive Bayes\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[86 14]\n",
      " [23 31]]\n",
      "\n",
      "Accuracy: 75.97%\n",
      "Precision: 68.89%\n",
      "Recall: 57.41%\n",
      "F1 Score: 62.63%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Confusion Matrix:\n",
      " [[84 16]\n",
      " [25 29]]\n",
      "\n",
      "Accuracy: 73.38%\n",
      "Precision: 64.44%\n",
      "Recall: 53.70%\n",
      "F1 Score: 58.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features (drop the 'Outcome' column)\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))  # First layer with 10 neurons\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Confusion Matrix:\n",
      " [[79 21]\n",
      " [24 30]]\n",
      "\n",
      "Accuracy: 70.78%\n",
      "Precision: 58.82%\n",
      "Recall: 55.56%\n",
      "F1 Score: 57.14%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1).values  # Features (convert to numpy array)\n",
    "y = data['Outcome'].values  # Target variable (convert to numpy array)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # Reshaping for LSTM\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))      # Reshaping for LSTM\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))  # LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Confusion Matrix:\n",
      " [[79 21]\n",
      " [26 28]]\n",
      "\n",
      "Accuracy: 69.48%\n",
      "Precision: 57.14%\n",
      "Recall: 51.85%\n",
      "F1 Score: 54.37%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1).values  # Features (convert to numpy array)\n",
    "y = data['Outcome'].values  # Target variable (convert to numpy array)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # Reshaping for LSTM\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))      # Reshaping for LSTM\n",
    "\n",
    "# Create the BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(X_train.shape[1], X_train.shape[2])))  # Bidirectional LSTM layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[83 17]\n",
      " [25 29]]\n",
      "\n",
      "Accuracy: 72.73%\n",
      "Precision: 63.04%\n",
      "Recall: 53.70%\n",
      "F1 Score: 58.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_diabetes_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = data.drop('Outcome', axis=1)  # Features\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create individual classifiers\n",
    "log_clf = LogisticRegression()\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "forest_clf = RandomForestClassifier()\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logistic', log_clf),\n",
    "        ('decision_tree', tree_clf),\n",
    "        ('random_forest', forest_clf)\n",
    "    ],\n",
    "    voting='hard'  # Use 'soft' for probability-based voting\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix and the evaluation metrics\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
